[package]
name = "open_clip_inference"
version = "0.2.5"
edition = "2024"
description = "Run OpenCLIP compatible embedding models via ONNX Runtime"
repository = "https://github.com/RuurdBijlsma/open-clip-rs"
license = "MIT"
keywords = ["clip", "onnx", "embedding", "inference", "computer-vision"]
categories = ["computer-vision"]
exclude = ["assets", "benches", "examples", "scripts", "tests"]

[dependencies]
ort = { version = "2.0.0-rc.11", features = ["ndarray"] }
ndarray = { version = "0.17.2", features = ["rayon"] }
image = "0.25.9"
tokenizers = { version = "0.22.2" }
rayon = "1.11.0"
thiserror = "2.0.18"
num_cpus = "1.17.0"
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.149"
bon = "3.8.2"
hf-hub = { version = "0.4.3", features = ["tokio"], optional = true }
fast_image_resize = { version = "6.0.0", optional = true }

[features]
default = ["download-binaries", "copy-dylibs", "hf-hub", "fast_image_resize"]
# crate features
hf-hub = ["dep:hf-hub"] # -> Enable downloading models from HuggingFace, relies on `tokio`
fast_image_resize = ["dep:fast_image_resize"] # -> Use `fast_image_resize` crate for vision preprocessing (resize)
# ort link features
download-binaries = ["ort/download-binaries"]
copy-dylibs = ["ort/copy-dylibs"]
load-dynamic = ["ort/load-dynamic"] # -> Load ONNXRuntime dynamically instead of statically
# ort execution providers
cuda = ["ort/cuda"]
tensorrt = ["ort/tensorrt"]
nvrtx = ["ort/nvrtx"]
xnnpack = ["ort/xnnpack"]
webgpu = ["ort/webgpu"]
directml = ["ort/directml"]
coreml = ["ort/coreml"]
migraphx = ["ort/migraphx"]
openvino = ["ort/openvino"]
onednn = ["ort/onednn"]
qnn = ["ort/qnn"]
cann = ["ort/cann"]
nnapi = ["ort/nnapi"]
tvm = ["ort/tvm"]
acl = ["ort/acl"]
armnn = ["ort/armnn"]
vitis = ["ort/vitis"]
rknpu = ["ort/rknpu"]
azure = ["ort/azure"]

[dev-dependencies]
color-eyre = "0.6.5"
criterion = { version = "0.8.1", features = ["html_reports"] }
tokio = "1.49.0"

[[bench]]
name = "model_bench"
harness = false